{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equação de Bellman\n",
    "\n",
    "A Equação de Bellman é um conceito fundamental no campo do Aprendizado por Reforço, que é uma abordagem de aprendizado de máquina em que um agente aprende a tomar decisões interativas para maximizar suas recompensas ao interagir com um ambiente. A equação leva o nome do matemático Richard Bellman, que contribuiu significativamente para a teoria dos processos de decisão estocásticos.\n",
    "\n",
    "A Equação de Bellman no contexto do Aprendizado por Reforço é uma fórmula que expressa como o valor esperado de um estado está relacionado aos valores esperados dos estados futuros, levando em consideração as recompensas imediatas e as possíveis transições de estado. Ela descreve como um agente deve avaliar o valor de um estado com base nas recompensas futuras esperadas, levando em conta as ações que o agente pode escolher.\n",
    "\n",
    "A formulação básica da Equação de Bellman é a seguinte:\n",
    "\n",
    "`V(s) = max_a [ R(s, a) + γ * Σ_s' [ P(s' | s, a) * V(s') ] ]`\n",
    "\n",
    "Onde:\n",
    "\n",
    "* V(s) é o valor esperado do estado s.\n",
    "* max_a denota a maximização sobre todas as ações possíveis a no estado s.\n",
    "* R(s, a) é a recompensa imediata obtida ao executar a ação a no estado s.\n",
    "* γ é o fator de desconto que pondera as recompensas futuras em relação às recompensas imediatas.\n",
    "* P(s' | s, a) é a probabilidade de transição para o estado s' a partir do estado s ao executar a ação a.\n",
    "* V(s') é o valor esperado do estado futuro s'.\n",
    "\n",
    "Em resumo, a Equação de Bellman é uma ferramenta crucial para avaliar o valor de diferentes estados e guiar as decisões do agente para maximizar suas recompensas ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Parâmetros do Jogo\n",
    "\n",
    "Aqui, definimos os parâmetros básicos do jogo, incluindo o tamanho do grid, o estado inicial do agente, o estado do objetivo (moeda) e a taxa de desconto (gamma) usada na equação de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar Valores de Estado (V)\n",
    "\n",
    "Inicializamos uma matriz V de tamanho (rows, cols) com zeros. Essa matriz representa os valores de estado do agente, ou seja, a estimativa de recompensa que o agente espera receber a partir de cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Função de Recompensa\n",
    "\n",
    "Aqui, definimos uma função de recompensa que retorna 10 se o estado for o estado do objetivo (moeda) e -1 caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Movimentos Possíveis\n",
    "\n",
    "Criamos uma lista de ações possíveis que o agente pode tomar: mover-se para cima, para baixo, para a esquerda ou para a direita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Aprendizado por Reforço\n",
    "\n",
    "Neste passo, implementamos o algoritmo de aprendizado por reforço usando a equação de Bellman. Realizamos iterações para atualizar os valores de estado (V) com base nas recompensas esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir a Política do Agente\n",
    "\n",
    "Aqui, definimos a política do agente, ou seja, a ação que o agente deve escolher em cada estado para maximizar sua recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Imprimir Valores Aprendidos para Cada Estado\n",
    "\n",
    "Este trecho de código imprime os valores de estado aprendidos para cada posição no grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imprimir a Política do Agente\n",
    "Este trecho de código imprime a política final do agente, mostrando a ação escolhida em cada posição do grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
